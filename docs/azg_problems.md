原项目问题（平局设置的考量之前甚至没有实现，动作空间的考虑以及网络的转化，很多东西没有实现，噪声，数据增强，温度使用的时间，u值设置等等，附文件，文件错误如MCTS视角默认为1，没有视角转化，无法学习），提供的功能，alphabeta的数据问题（是否使用调节，历史删除设置），其他agent，如何比较（两种显示提供），如何训练，参数如何调节，Elo结果解释，当前参数设置原因（可以附文件细说），详细项目文件一一介绍和项目修改的详细日志，串行设置训练设置

# 决定使用此项目
在开始发现alphazero-general这个项目，考虑到能够节约时间，并且这也是难得的学习一个成熟项目的机会所以选择基于这个项目解决我的问题。
# 一开始对于现在这个项目的规划
原来没有真正深入阅读过GitHub中一些高fork和star的项目，所以以为这个项目很成熟，同时AlphaZero的原论文中没有包含太多的架构设计，所以就决定通过阅读这个实现的代码来了解具体的实现。开始我还担心其已经实现了AlphaZero的设计，同时也提供了Tafl的设置，所以我可能做不了太多，最多是根据Bannerlord的规则进行调整，然后增加Elo分析，完整的恢复和可视化，最后添加一些有效的部件，然后做消融实验。并觉得这个应该会比较简单，花不了太长时间。后来才发现这个项目对于AlphaZero实现可以说是漏洞百出，尤其是后期我在AlphaGoZero的论文和AlphaZero的补充材料中了解到了整体的设计细节后。
# azg原项目存在的问题和解决方案
1. **可跑错误**，开始项目不可跑
   1. train部分对于棋盘没有做格式转化而直接输入网络，应该转化为矩阵，然后处理为数组。
   2. MCTS树用于标记各个状态的键值表示有问题，只使用了棋盘的局面，而没有是否终局的标志，使得无法区分一个局面是能够继续进行还是已经因为如超过步数限制等原因平局而结束，如果先记录了继续，则可能会在该超时终局时陷入死循环。所以应该添加终局值，同时还要给目前的步数和进展值，谁在下，可能担心因为太稀疏导致难以利用，但是不用担心，因为价值的复用在AlphaZero中是通过网络来实现的。
2. **架构问题**，整体项目的架构设计存在问题
   1. Arena有**平局**的项，但是整体是没有平局的设计的，至少Tafl没有，其以1为胜-1为负，0为没有结束，根本没有考虑到平局如何表示，但偏偏所有规则都是基于这种表示的，所以出于工程考量我用一个极小值表示了平局。
   2. 其次对于**动作空间**AlphaZero的补充材料中，相似的国际象棋使用的是所有棋子可能的路径即可能的直线，斜线，日等等，而对于这个明显更简单的棋明明动作空间就是在棋子上下左右四个方位走直线，对于不同棋可能要优化，但是其选择的是一种最通用的，即从任意位置到任意位置，这使得动作空间大大增加，以这里我解决的Tablut为例，从n^2^ &times; 4 &times; 8，增长到n^4^，n=9，这会增加网络预测的难度，同时所有规则设置都是基于这个动作空间，难以修改，我只能在网络输出时从小空间转化到大空间，在训练时将数据从大空间压缩到小空间，很丑陋。
3. **错误实现问题**，一些实现直接是错的，无法使网络学习
   1. **视角转化问题**，MCTS中Search部分全部以白棋的视角写的，根本没有做转化，同时棋局也没有做视角转化，这使得网络和MCTS根本无法区分面对一盘棋是谁在下，根本无法学习。 而我的解决方案是设置两个标志，即棋盘是否翻转棋子，是否翻转王，如果是从黑棋的视角则要翻转，这个不会实际修改棋子的值，因为游戏规则的设置等都是基于棋子的值，所以我们仅仅根据标志修改转化为矩阵的值，这样MCTS的键值和网络的输入可以区分视角。
   2. **合法动作问题**，_isLegalMove在边沿的越界检测中少了一个等号，应是大于等于。
   3. **整数动作和四元动作转化**，原实现在0处存在问题。
   4. **无可行动作判负**，没有可行动作时应该直接判负，但这里直接给了一个动作。
   5. **Temp**，训练时温度应该一直为1，鼓励探索，而这里仅在前面几步使温度为1。
   6. **数据利用**，原本是并行的，随机选择，但这里虽然首先是串行，但是改造的也很不合理，其会每次将池子中所有数据训练，同时训练多个epoch，这应该很容易过拟合。所以我选择先得到训练数据，然后将池子中的数据全部给网络，网络建立数据集，随机抽取，一个step跑一个Batchsize，然后跑1000个Batchsize后进行测试，详细的为了设置参数的计算我放在[这里](/docs/Parameters_Manage.md)。
   7. **网络输入**，原论文的补充材料中的Representation部分提到要手动分解输入，而不是直接将局面的原始表示给网络。
4. **无实现问题**，写了架构，但是没有实际实现
   1. **数据增强**，对于这个Tafl，可以旋转和镜像做数据增强，因为其有对称性，但是其没有实现，同时实现的位置也不合理，放在训练部分更好，因为可以节省开始处理的时间，同时节约存储空间，最重要的是降低数据的相关性。
   2. **平局规则**，首先是根本没有考虑平局，那么相应的步数超限平局，三次循环平局，没有进展平局就都没有实现。
   3. **狄利克雷噪声**，这个噪声没有实现，在MCTS树的根部。
   4. **L2损失**，对于网络参数没有实现L2损失。

# 其他修改
1. **UCB，上置信界**，我使用的是MuZero的那种动态U值。
2. **alphabeta数据**，为了节约时间快速学习我提供了让2层alphabeta作为老师，提供训练数据让网络迅速进行了学习。
